# 参考人物機能 - システムの動作フロー

**作成日**: 2025年11月15日

---

## 🔄 システムの動作（全体像）

### 参考人物なしの場合（従来）

```
【ユーザー操作】
1. 服の画像をアップロード

【システム処理】
2. Gemini APIに送信:
   [服の画像]
   + "Create a NEW fashion model wearing this"

【Gemini AI】
3. 服を見る
4. ランダムに新しい人物を生成
5. その人物に服を着せて出力

【結果】
毎回異なる新しいモデル
```

---

### 参考人物ありの場合（新機能）

```
【ユーザー操作】
1. 参考人物の画像をアップロード
2. 服の画像をアップロード

【システム処理】
3. Gemini APIに送信:
   [参考人物の画像]  ← 1枚目
   [服の画像]         ← 2枚目
   + "Look at image 1 (PERSON), image 2 (CLOTHES).
      Put these clothes on THE SAME PERSON from image 1"

【Gemini AI】
4. 画像1を見る → 人物を認識
5. 画像2を見る → 服を認識
6. ??? ← ここが問題！
7. 画像を生成して出力

【結果】
現状: 新しいモデルが生成される（参考人物が保持されない）
```

---

## 🔍 問題の核心

### Gemini AIの内部処理（推測）

```
パターン1（期待される動作）:
  参考人物の画像を「ベース」として使用
    ↓
  服の部分だけを変更
    ↓
  同じ人物が服を着た画像

パターン2（現在の動作）:
  参考人物の画像を「参考資料」として使用
    ↓
  参考人物に似た新しい人物を生成
    ↓
  新しい人物が服を着た画像（似ているが別人）
```

**現状**: パターン2で動作していると思われる ⚠️

---

## 🎯 本質的な問題

### Gemini 2.5 Flash Imageの制約

**Gemini 2.5 Flash Image**は：

```
✅ できること:
   - 複数の画像を見て理解する
   - 画像から新しい画像を生成する
   - テキスト指示に従って生成する

❌ できないこと（可能性）:
   - 既存画像の「一部だけ」を変更する
   - 特定人物のアイデンティティを完全に保持する
   - 画像の編集・合成
```

### なぜそうなのか

**画像生成モデル** vs **画像編集モデル**:

```
【画像生成モデル】（Gemini 2.5 Flash Image）
  - ゼロから新しい画像を作る
  - 複数の画像を「参考」にできる
  - しかし「そのまま使用」はしない
  
【画像編集モデル】（DALL-E Edit, Inpainting等）
  - 既存画像をベースにする
  - 指定した部分だけを変更
  - 他の部分はそのまま保持
```

**Geminiは前者（生成）です** ⚠️

---

## 📊 実際のデータフロー

### コードレベルの動作

```python
# 1. UIで参考人物を選択
self.reference_person_image = "C:/path/person.jpg"

# 2. 生成開始
adapter.set_reference_person(self.reference_person_image)

# 3. Geminiアダプターで処理
prompt_parts = []
prompt_parts.append(PIL.Image.open(参考人物画像))  # 画像1
prompt_parts.append(PIL.Image.open(服の画像))      # 画像2
prompt_parts.append(プロンプトテキスト)

# 4. Gemini APIに送信
response = model.generate_content(prompt_parts)

# 5. Geminiの内部処理（ブラックボックス）
#    ↓
#    ??? 
#    ↓

# 6. 画像が返ってくる
generated_image = response.candidates[0].content.parts[0].inline_data
```

**問題**: ステップ5のGeminiの内部処理が、我々の期待通りに動作していない

---

## 💡 Geminiが実際にやっていること（推測）

### 現在の動作（推測）

```
入力:
  画像1: [女性の全身写真]
  画像2: [ジャケットの写真]
  プロンプト: "Put this jacket on THE SAME PERSON from image 1"

Geminiの処理:
  1. 画像1を分析: 
     - 女性、黒髪、スリム、20代、アジア系...
  
  2. 画像2を分析:
     - 赤いジャケット、フード付き、ロゴあり...
  
  3. プロンプトを理解:
     - 「同じ人物」に「このジャケット」を着せる
  
  4. 新しい画像を生成:
     - 画像1の特徴を「参考」にする
     - しかし画像1をそのまま使うわけではない
     - 類似した新しい人物を生成 ← ここが問題
     - その人物にジャケットを着せる

結果:
  参考人物に「似ているが別人」の画像
```

---

## 🔬 技術的な理由

### なぜ「そのまま」使えないのか

**Gemini 2.5 Flash Image**の技術的アーキテクチャ:

```
【Text-to-Image生成モデル】
  テキスト → 画像

【Image-to-Image生成モデル】
  画像 + テキスト → 新しい画像
  ↑
  画像は「参考」や「スタイル」として使用
  画像をそのまま編集するわけではない
```

**類似例**:
- Stable Diffusion: image-to-imageは「参考」として使用
- Midjourney: 参考画像は「スタイル」として使用
- どちらも「そのまま保持」するわけではない

---

## 🎯 プロンプトの問題なのか？

### 回答：**プロンプトだけの問題ではありません**

**複合的な問題**:

1. **モデルの制約（70%）**:
   - Gemini 2.5 Flash Imageの技術的制約
   - 画像編集ではなく画像生成に特化

2. **プロンプトの問題（20%）**:
   - より強力なプロンプトで改善の余地あり
   - しかし完全な解決は困難

3. **画像の問題（10%）**:
   - 参考人物画像の品質
   - 服の画像の品質

---

## 💡 解決策

### 短期的な改善（プロンプト強化）

**既に実施済み**:
- より詳細で明確な指示
- 「EXACT SAME」の強調
- 思考プロセスの誘導

**期待される効果**:
- 類似度が20% → 60-70%に向上（推測）
- しかし100%の保持は困難

### 中期的な解決（画像編集AIの導入）

**選択肢**:

#### オプション1: Stability AI Inpainting

```python
# 服の部分だけを塗り替える
result = stability_inpainting_api(
    image=参考人物画像,
    mask=服の領域マスク,
    prompt="wearing this jacket"
)
```

**メリット**:
- 参考人物を完全に保持
- 服の部分だけを変更

**デメリット**:
- 追加API必要
- マスク生成が必要

#### オプション2: OpenAI DALL-E Edit

```python
result = openai.images.edit(
    image=参考人物画像,
    mask=服の領域マスク,
    prompt="wearing a red jacket"
)
```

**メリット**:
- 人物保持の精度が高い

**デメリット**:
- 追加APIキー必要

---

## 📊 現実的な評価

### 現在のGemini実装で達成できること

| 項目 | 達成度 |
|------|--------|
| 参考人物の送信 | ✅ 100% |
| Geminiの認識 | ✅ 100% |
| 類似した人物の生成 | 🟡 60-70% |
| **完全な人物保持** | ❌ **20-30%** |
| 服の再現 | ✅ 90-95% |

### 結論

**現在のGemini 2.5 Flash Image実装では**:
- ✅ 「参考人物に似た」モデルは生成できる
- ❌ 「参考人物そのもの」を保持することは困難

**理由**: 
Geminiが画像編集ではなく、画像生成に特化しているため

---

## 🚀 推奨される対応

### オプションA: 現状を受け入れる

機能の説明を変更:
```
「参考人物の画像を指定すると、
 その人物に似たモデルに服を着せた画像を生成します」
```

### オプションB: 画像編集AIを追加実装

Stability AI Inpainting や DALL-E Edit を追加実装し、
より高精度な人物保持を実現

### オプションC: プロンプトをさらに強化

現在のプロンプトでテストして、改善の余地があればさらに調整

---

## 🎊 まとめ

### システムの動作

```
1. ユーザーが参考人物画像を選択 ✅
2. システムが画像を読み込み ✅
3. Gemini APIに送信 ✅
4. Geminiが画像を認識 ✅
5. Geminiが新しい画像を生成 ✅
6. しかし「そのまま保持」はしない ← ここが制約
```

### 問題の本質

**プロンプトの問題ではなく、Geminiの技術的制約です**

- Geminiは「画像生成」モデル
- 「画像編集」には限界がある
- 完全な人物保持は困難

---

**アプリケーションを再起動して、強化されたプロンプトで結果を確認してください。**

改善が見られない場合は、画像編集特化AIの導入をご検討ください。
